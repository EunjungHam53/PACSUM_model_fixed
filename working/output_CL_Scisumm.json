{
    "P04-1036_sweta": "Finding Predominant Word Senses in Untagged Text The paper is structured as follows. We discuss our method in the following section. For each noun we considered the co-occurring verbs in the direct object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations. We trivially labelled all monosemous items. Many of the articles are economy related, but several other topics are included too. The SFC contains an economy label and a sports label. Our approach is complementary to this. Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD. This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies, UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.",
    "P11-1060_swastika": "Learning Dependency-Based Compositional Semantics What is the total population of the ten largest capitals in the US? Which one should we use? Let V be the set of all values, which includes primitives (e.g., 3, CA \u2208 V) as well as sets and tuples formed from other values (e.g., 3, {3, 4, 7}, (CA, {5}) \u2208 V). We say a value v is consistent for a node x if there exists a solution that assigns v to x. We write d as ((A; (ri, bi, ci); ... ; (rn, bn, cn))). Formally, p\u03b8(z  |x) \u221d e\u03c6(x,z)T\u03b8, where \u03b8 and \u03c6(x, z) are parameter and feature vectors, respectively. We therefore resort to beam search. We tested our system on two standard datasets, GEO and JOBS. Our features as soft preferences.",
    "W99-0613_vardha": "Unsupervised Models for Named Entity Classification Collins We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The first modification \u2014 cautiousness \u2014 is a relatively minor change. Fort= 1,...,T: One implementation issue deserves some elaboration. Again, this deserves further investigation. Unfortunately, modifying the model to account for these kind of dependencies is not at all straightforward. Of these cases, 38 were temporal expressions (either a day of the week or month of the year). We excluded these from the evaluation as they can be easily identified with a list of days/months.",
    "W06-2932_swastika": "Multilingual Dependency Analysis with a Two-Stage Discriminative Parser That system uses MIRA, an online large-margin learning algorithm, to compute model parameters. These features are highly important to overall accuracy since they eliminate unlikely scenarios such as a preposition modifying a noun not directly to its left, or a noun modifying a verb with another verb occurring between them. We used the following: dependent have identical values? The benefit of each of these is shown in Table 2. The major contribution was in helping to distinguish subjects, objects and other dependents of main verbs, which is the most common labeling error. Similar improvements are common across all languages, though not as dramatic. We confirmed that the main clause is often misidentified in multi-clause sentences, or that one of several conjoined clauses is incorrectly taken as the main clause. The fact that Arabic has only 1500 training instances might also be problematic. In the future we plan to extend these models in two ways.",
    "A00-2018_sweta": "A Maximum-Entropy-Inspired Parser * Whenever it is clear to which constituent we are referring we omit the (c) in, e.g., h(c). In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does. Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a, H) for 0 < i < j: Here go(a, H) = 11Z (H) and gi(a, H) = eAi(a,H) fi(\u00b0,11). In many cases this is clearly warranted. (Our experience is that rather than requiring 50 or so iterations, three suffice.) (Actually, we use a minor variant described in [4].) The Experiment The other four equations can be found in a longer version of this paper available on the author's website (www.cs.brown.eduhiec). This is due to the importance of this factor in parsing, as noted in, e.g., [14].",
    "E03-1005_sweta": "An Efficient Implementation of a New DOP Model Introduction: A Little History in Fujisaki et al. 1989; Black et al. 1992, 1993; Briscoe and I Thanks to Ivan Sag for this pun. This approach has now gained wide usage, as exemplified by the work of Collins (1996, 1999), Charniak (1996, 1997), Johnson (1998), Chiang (2000), and many others. Cross-validation is needed to avoid this problem. There are several ways to fix this problem. 40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set. As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks. We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems.",
    "P08-1028_aakansha": "Vector-based Models of Semantic Composition In fact, the commonest method for combining the vectors is to average them. (1) a. It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. b. Related Work The proposal is not merely notational. Materials and Design The pretest was completed by 53 participants. The study was conducted remotely over the Internet using Webexp4, a software package designed for conducting psycholinguistic studies over the web. A Wilcoxon rank sum test confirmed that the difference is statistically significant (p < 0.01).",
    "A97-1014_swastika": "An Annotation Scheme for Free Word Order Languages (Marcus et al., 1994). , 1996)). (Hudson, 1984). Further differences concern the attachment of the degree modifier sehr. An explicit coordinating conjunction need not be present. Separable verb prefixes are labeled SVP. The corpus is stored in a SQL database. The tagger rates 90% of all assignments as reliable and carries them out fully automatically. This work is part of the DFG Sonderforschungsbereich 378 Rcsource-Adaptim Cogniiivc Proccsses, We wish to thank Tania Avgustinova, Berthold Crysmann, Lars Konieczny, Stephan Oepen, Karel Oliva., Christian Weil3 and two anonymous reviewers for their helpful comments on the content of this paper.",
    "E03-1005_aakansha": "An Efficient Implementation of a New DOP Model Introduction: A Little History in Fujisaki et al. 1989; Black et al. 1992, 1993; Briscoe and I Thanks to Ivan Sag for this pun. This approach has now gained wide usage, as exemplified by the work of Collins (1996, 1999), Charniak (1996, 1997), Johnson (1998), Chiang (2000), and many others. Cross-validation is needed to avoid this problem. There are several ways to fix this problem. 40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set. As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks. We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems.",
    "A97-1014_sweta": "An Annotation Scheme for Free Word Order Languages (Marcus et al., 1994). , 1996)). (Hudson, 1984). Further differences concern the attachment of the degree modifier sehr. An explicit coordinating conjunction need not be present. Separable verb prefixes are labeled SVP. The corpus is stored in a SQL database. The tagger rates 90% of all assignments as reliable and carries them out fully automatically. This work is part of the DFG Sonderforschungsbereich 378 Rcsource-Adaptim Cogniiivc Proccsses, We wish to thank Tania Avgustinova, Berthold Crysmann, Lars Konieczny, Stephan Oepen, Karel Oliva., Christian Weil3 and two anonymous reviewers for their helpful comments on the content of this paper.",
    "W11-2123_aakansha": "KenLM: Faster and Smaller Language Model Queries The structure uses linear probing hash tables and is designed for speed. Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations. Entries landing in the same bucket are said to collide. Related Work Alon Lavie advised on this work. Juri Ganitkevitch answered questions about Joshua. 0750271 and by the DARPA GALE program. IRST is not threadsafe.",
    "P08-1102_swastika": "Cascaded Linear Model However, as such features are generated dynamically during the decoding procedure, two limitation arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications. We will describe it in detail in Section 4. With precision P and recall R, the balance F-measure is defined as: F = 2PR/(P + R). We found that LEX outperforms NON-LEX with a margin of about 0.002 at each iteration, and its learning curve reaches a tableland at iteration 7. We will investigate these problems in the following work. This work was done while L. H. was visiting CAS/ICT. The authors were supported by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No. 2006AA010108 (W. J., Q. L., and Y. L.), and by NSF ITR EIA-0205456 (L. H.). We would also like to Hwee-Tou Ng for sharing his code, and Yang Liu and Yun Huang for suggestions.",
    "W99-0613_sweta": "Unsupervised Models for Named Entity Classification Collins We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The first modification \u2014 cautiousness \u2014 is a relatively minor change. Fort= 1,...,T: One implementation issue deserves some elaboration. Again, this deserves further investigation. Unfortunately, modifying the model to account for these kind of dependencies is not at all straightforward. Of these cases, 38 were temporal expressions (either a day of the week or month of the year). We excluded these from the evaluation as they can be easily identified with a list of days/months.",
    "A00-2030_aakansha": "A Novel Use of Statistical Parsing to Extract Information from Text Manually creating sourcespecific training data for syntax was not required. For each organization in an article, one must identify all of its names as used in the article, its type (corporation, government, or other), and any significant description of it. Integrated Sentential Processing Post-modifier constituents for the PER/NP. Experimental Results 1 Conclusions The work reported here was supported in part by the Defense Advanced Research Projects Agency. Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062, F30602-97-C-0096, and 4132-BBN-001. We thank Michael Collins of the University of Pennsylvania for his valuable suggestions.",
    "P87-1015_sweta": "CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS* Pumping t2 will change only one branch and leave the other branch unaffected. An ATM has two types of states, existential and universal. In an existential state an ATM behaves like a nondeterministic TM, accepting if one of the applicable moves leads to acceptance; in an universal state the ATM accepts if all the applicable moves lead to acceptance. An ATM may be thought of as spawning independent processes for each applicable move. A k-tape ATM, M, has a read-only input tape and k read-write work tapes. M spawns as many processes as there are ways of breaking up ri , .. \u2022 , zt, and rules with A on their left-hand-side. Each spawned process must check if xi , , xn, and , yn, can be derived from B and C, respectively. To do this, the x's and y's are stored in the next 2ni + 2n2 tapes, and M goes to a universal state. Two processes are spawned requiring B to derive z,.., and C to derive yi , , y,.",
    "W99-0623_sweta": "Exploiting Diversity in Natural Language Processing: Combining Parsers Two general approaches are presented and two combination techniques are described for each approach. Both parametric and non-parametric models are explored. The theory has also been validated empirically. These two principles guide experimentation in this framework, and together with the evaluation measures help us decide which specific type of substructure to combine. For example, we may have semantic information (e.g. database query operations) associated with the productions in a grammar. The computation of Pfr1(c)1Mi M k (C)) has been sketched before in Equations 1 through 4. This is not an oversight. The first two rows of the table are baselines. This work was funded by NSF grant IRI-9502312.",
    "P11-1060_aakansha": "Learning Dependency-Based Compositional Semantics What is the total population of the ten largest capitals in the US? Which one should we use? Let V be the set of all values, which includes primitives (e.g., 3, CA \u2208 V) as well as sets and tuples formed from other values (e.g., 3, {3, 4, 7}, (CA, {5}) \u2208 V). We say a value v is consistent for a node x if there exists a solution that assigns v to x. We write d as ((A; (ri, bi, ci); ... ; (rn, bn, cn))). Formally, p\u03b8(z  |x) \u221d e\u03c6(x,z)T\u03b8, where \u03b8 and \u03c6(x, z) are parameter and feature vectors, respectively. We therefore resort to beam search. We tested our system on two standard datasets, GEO and JOBS. Our features as soft preferences.",
    "P11-1060_sweta": "Learning Dependency-Based Compositional Semantics What is the total population of the ten largest capitals in the US? Which one should we use? Let V be the set of all values, which includes primitives (e.g., 3, CA \u2208 V) as well as sets and tuples formed from other values (e.g., 3, {3, 4, 7}, (CA, {5}) \u2208 V). We say a value v is consistent for a node x if there exists a solution that assigns v to x. We write d as ((A; (ri, bi, ci); ... ; (rn, bn, cn))). Formally, p\u03b8(z  |x) \u221d e\u03c6(x,z)T\u03b8, where \u03b8 and \u03c6(x, z) are parameter and feature vectors, respectively. We therefore resort to beam search. We tested our system on two standard datasets, GEO and JOBS. Our features as soft preferences.",
    "P08-1028_swastika": "Vector-based Models of Semantic Composition In fact, the commonest method for combining the vectors is to average them. (1) a. It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. b. Related Work The proposal is not merely notational. Materials and Design The pretest was completed by 53 participants. The study was conducted remotely over the Internet using Webexp4, a software package designed for conducting psycholinguistic studies over the web. A Wilcoxon rank sum test confirmed that the difference is statistically significant (p < 0.01).",
    "W06-2932_vardha": "Multilingual Dependency Analysis with a Two-Stage Discriminative Parser That system uses MIRA, an online large-margin learning algorithm, to compute model parameters. These features are highly important to overall accuracy since they eliminate unlikely scenarios such as a preposition modifying a noun not directly to its left, or a noun modifying a verb with another verb occurring between them. We used the following: dependent have identical values? The benefit of each of these is shown in Table 2. The major contribution was in helping to distinguish subjects, objects and other dependents of main verbs, which is the most common labeling error. Similar improvements are common across all languages, though not as dramatic. We confirmed that the main clause is often misidentified in multi-clause sentences, or that one of several conjoined clauses is incorrectly taken as the main clause. The fact that Arabic has only 1500 training instances might also be problematic. In the future we plan to extend these models in two ways.",
    "E03-1005_swastika": "An Efficient Implementation of a New DOP Model Introduction: A Little History in Fujisaki et al. 1989; Black et al. 1992, 1993; Briscoe and I Thanks to Ivan Sag for this pun. This approach has now gained wide usage, as exemplified by the work of Collins (1996, 1999), Charniak (1996, 1997), Johnson (1998), Chiang (2000), and many others. Cross-validation is needed to avoid this problem. There are several ways to fix this problem. 40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set. As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks. We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems.",
    "P08-1102_aakansha": "Cascaded Linear Model However, as such features are generated dynamically during the decoding procedure, two limitation arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications. We will describe it in detail in Section 4. With precision P and recall R, the balance F-measure is defined as: F = 2PR/(P + R). We found that LEX outperforms NON-LEX with a margin of about 0.002 at each iteration, and its learning curve reaches a tableland at iteration 7. We will investigate these problems in the following work. This work was done while L. H. was visiting CAS/ICT. The authors were supported by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No. 2006AA010108 (W. J., Q. L., and Y. L.), and by NSF ITR EIA-0205456 (L. H.). We would also like to Hwee-Tou Ng for sharing his code, and Yang Liu and Yun Huang for suggestions.",
    "P87-1015_vardha": "CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS* Pumping t2 will change only one branch and leave the other branch unaffected. An ATM has two types of states, existential and universal. In an existential state an ATM behaves like a nondeterministic TM, accepting if one of the applicable moves leads to acceptance; in an universal state the ATM accepts if all the applicable moves lead to acceptance. An ATM may be thought of as spawning independent processes for each applicable move. A k-tape ATM, M, has a read-only input tape and k read-write work tapes. M spawns as many processes as there are ways of breaking up ri , .. \u2022 , zt, and rules with A on their left-hand-side. Each spawned process must check if xi , , xn, and , yn, can be derived from B and C, respectively. To do this, the x's and y's are stored in the next 2ni + 2n2 tapes, and M goes to a universal state. Two processes are spawned requiring B to derive z,.., and C to derive yi , , y,.",
    "P08-1102_sweta": "Cascaded Linear Model However, as such features are generated dynamically during the decoding procedure, two limitation arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications. We will describe it in detail in Section 4. With precision P and recall R, the balance F-measure is defined as: F = 2PR/(P + R). We found that LEX outperforms NON-LEX with a margin of about 0.002 at each iteration, and its learning curve reaches a tableland at iteration 7. We will investigate these problems in the following work. This work was done while L. H. was visiting CAS/ICT. The authors were supported by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No. 2006AA010108 (W. J., Q. L., and Y. L.), and by NSF ITR EIA-0205456 (L. H.). We would also like to Hwee-Tou Ng for sharing his code, and Yang Liu and Yun Huang for suggestions.",
    "P08-1043_sweta": "A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing \u201cin the shadow\u201d. Here we do not submit to this view. Parameter Estimation Our smoothing procedure takes into account all the aforementioned aspects and works as follows. Models that employ this strategy are denoted hsp. We currently simulate this by crafting a WCFG and feeding it to BitPar. Experimental Setup Discussion and Conclusion We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal.",
    "P05-1013_vardha": "Pseudo-Projective Dependency Parsing The rest of the paper is structured as follows. We then evaluate the approach in two steps. If (wi, r, wj) E A, we say that wi is the head of wj and wj a dependent of wi. To explore this tradeoff, we have performed experiments with three different encoding schemes (plus a baseline), which are described schematically in Table 1. In approaching this problem, a variety of different methods are conceivable, including a more or less sophisticated use of machine learning. In the present study, we limit ourselves to an algorithmic approach, using a deterministic breadthfirst search. The results are given in Table 4. All improvements over the baseline are statistically significant beyond the 0.01 level (McNemar\u2019s test). This work was supported in part by the Swedish Research Council (621-2002-4207).",
    "W11-2123_swastika": "KenLM: Faster and Smaller Language Model Queries The structure uses linear probing hash tables and is designed for speed. Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations. Entries landing in the same bucket are said to collide. Related Work Alon Lavie advised on this work. Juri Ganitkevitch answered questions about Joshua. 0750271 and by the DARPA GALE program. IRST is not threadsafe.",
    "P05-1013_swastika": "Pseudo-Projective Dependency Parsing The rest of the paper is structured as follows. We then evaluate the approach in two steps. If (wi, r, wj) E A, we say that wi is the head of wj and wj a dependent of wi. To explore this tradeoff, we have performed experiments with three different encoding schemes (plus a baseline), which are described schematically in Table 1. In approaching this problem, a variety of different methods are conceivable, including a more or less sophisticated use of machine learning. In the present study, we limit ourselves to an algorithmic approach, using a deterministic breadthfirst search. The results are given in Table 4. All improvements over the baseline are statistically significant beyond the 0.01 level (McNemar\u2019s test). This work was supported in part by the Swedish Research Council (621-2002-4207).",
    "W06-2932_sweta": "Multilingual Dependency Analysis with a Two-Stage Discriminative Parser That system uses MIRA, an online large-margin learning algorithm, to compute model parameters. These features are highly important to overall accuracy since they eliminate unlikely scenarios such as a preposition modifying a noun not directly to its left, or a noun modifying a verb with another verb occurring between them. We used the following: dependent have identical values? The benefit of each of these is shown in Table 2. The major contribution was in helping to distinguish subjects, objects and other dependents of main verbs, which is the most common labeling error. Similar improvements are common across all languages, though not as dramatic. We confirmed that the main clause is often misidentified in multi-clause sentences, or that one of several conjoined clauses is incorrectly taken as the main clause. The fact that Arabic has only 1500 training instances might also be problematic. In the future we plan to extend these models in two ways.",
    "W99-0613_swastika": "Unsupervised Models for Named Entity Classification Collins We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The first modification \u2014 cautiousness \u2014 is a relatively minor change. Fort= 1,...,T: One implementation issue deserves some elaboration. Again, this deserves further investigation. Unfortunately, modifying the model to account for these kind of dependencies is not at all straightforward. Of these cases, 38 were temporal expressions (either a day of the week or month of the year). We excluded these from the evaluation as they can be easily identified with a list of days/months.",
    "D09-1092_vardha": "Polylingual Topic Models The authors thank Limin Yao, who was involved in early stages of this project. This work was supported in part by the Center for Intelligent Information Retrieval, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant number IIS-0326249, and in part by Army prime contract number W911NF-07-1-0216 and University of Pennsylvania subaward number 103548106, and in part by National Science Foundation under NSF grant #CNS-0619337. Any opinions, findings and conclusions or recommendations expressed in this material are the authors\u2019 and do not necessarily reflect those of the sponsor. These tasks can either be accomplished by averaging over samples of \u03a61, . We also remove the twenty-five most frequent word types for efficiency reasons. These tend to be very short, formulaic parliamentary responses, however. Cosine-based rankings are significantly worse. We preprocessed the data by removing tables, references, images and info-boxes. Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission \u03b4\u03b9\u03b1\u03c3\u03c4\u03b7\u03bc\u03b9\u03ba\u03cc sts nasa \u03b1\u03b3\u03b3\u03bb small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimm\u0160inen space lento spatiale mission orbite mars satellite spatial \u05ea\u05d9\u05e0\u05db\u05d5\u05ea \u05d0 \u05e8\u05d5\u05d3\u05db \u05dc\u05dc \u05d7 \u05e5\u05e8 \u05d0\u05d4 \u05dc\u05dc\u05d7\u05d4 spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0441\u043e\u044e\u0437 \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0441\u043f\u0443\u0442\u043d\u0438\u043a \u0441\u0442\u0430\u043d\u0446\u0438\u0438 uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1\u03c2 \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1 de \u03b9\u03c3\u03c0\u03b1\u03bd\u03cc\u03c2 \u03bd\u03c4\u03b5 \u03bc\u03b1\u03b4\u03c1\u03af\u03c4\u03b7 de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpa\u0144ski hiszpanii la juan y \u0434\u0435 \u043c\u0430\u0434\u0440\u0438\u0434 \u0438\u0441\u043f\u0430\u043d\u0438\u0438 \u0438\u0441\u043f\u0430\u043d\u0438\u044f \u0438\u0441\u043f\u0430\u043d\u0441\u043a\u0438\u0439 de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae\u03c2 \u03c0\u03bf\u03af\u03b7\u03c3\u03b7 \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae \u03ad\u03c1\u03b3\u03bf \u03c0\u03bf\u03b9\u03b7\u03c4\u03ad\u03c2 \u03c0\u03bf\u03b9\u03ae\u03bc\u03b1\u03c4\u03b1 poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.",
    "J01-2004_swastika": "This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition  The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling  A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers  A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity  Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model  A small recognition experiment also demonstrates the utility of the model The author wishes to thank Mark Johnson for invaluable discussion, guidance, and moral support over the course of this project. Many thanks also to Eugene Charniak for the use of certain grammar training routines, and for an enthusiastic interest in the project. Thanks also to four anonymous reviewers for valuable and insightful comments, and to Ciprian Chelba, Sanjeev Khudanpur, and Frederick Jelinek for comments and suggestions. Finally, the author would like to express his appreciation to the participants of discussions during meetings of the Brown",
    "A97-1014_vardha": "An Annotation Scheme for Free Word Order Languages (Marcus et al., 1994). , 1996)). (Hudson, 1984). Further differences concern the attachment of the degree modifier sehr. An explicit coordinating conjunction need not be present. Separable verb prefixes are labeled SVP. The corpus is stored in a SQL database. The tagger rates 90% of all assignments as reliable and carries them out fully automatically. This work is part of the DFG Sonderforschungsbereich 378 Rcsource-Adaptim Cogniiivc Proccsses, We wish to thank Tania Avgustinova, Berthold Crysmann, Lars Konieczny, Stephan Oepen, Karel Oliva., Christian Weil3 and two anonymous reviewers for their helpful comments on the content of this paper.",
    "W11-2123_vardha": "KenLM: Faster and Smaller Language Model Queries The structure uses linear probing hash tables and is designed for speed. Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations. Entries landing in the same bucket are said to collide. Related Work Alon Lavie advised on this work. Juri Ganitkevitch answered questions about Joshua. 0750271 and by the DARPA GALE program. IRST is not threadsafe.",
    "W06-3114_aakansha": "We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back  Evaluation was done automatically using the BLEU score and manually on fluency and adequacy Training and testing is based on the Europarl corpus. To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources. One may argue with these efforts on normalization, and ultimately their value should be assessed by assessing their impact on inter-annotator agreement. So, who won the competition? This actually happens quite frequently (more below), so that the rankings are broad estimates. So, this was a surprise element due to practical reasons, not malice. It rewards matches of n-gram sequences, but measures only at most indirectly overall grammatical coherence. This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No.",
    "D09-1092_swastika": "Polylingual Topic Models The authors thank Limin Yao, who was involved in early stages of this project. This work was supported in part by the Center for Intelligent Information Retrieval, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant number IIS-0326249, and in part by Army prime contract number W911NF-07-1-0216 and University of Pennsylvania subaward number 103548106, and in part by National Science Foundation under NSF grant #CNS-0619337. Any opinions, findings and conclusions or recommendations expressed in this material are the authors\u2019 and do not necessarily reflect those of the sponsor. These tasks can either be accomplished by averaging over samples of \u03a61, . We also remove the twenty-five most frequent word types for efficiency reasons. These tend to be very short, formulaic parliamentary responses, however. Cosine-based rankings are significantly worse. We preprocessed the data by removing tables, references, images and info-boxes. Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission \u03b4\u03b9\u03b1\u03c3\u03c4\u03b7\u03bc\u03b9\u03ba\u03cc sts nasa \u03b1\u03b3\u03b3\u03bb small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimm\u0160inen space lento spatiale mission orbite mars satellite spatial \u05ea\u05d9\u05e0\u05db\u05d5\u05ea \u05d0 \u05e8\u05d5\u05d3\u05db \u05dc\u05dc \u05d7 \u05e5\u05e8 \u05d0\u05d4 \u05dc\u05dc\u05d7\u05d4 spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0441\u043e\u044e\u0437 \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0441\u043f\u0443\u0442\u043d\u0438\u043a \u0441\u0442\u0430\u043d\u0446\u0438\u0438 uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1\u03c2 \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1 de \u03b9\u03c3\u03c0\u03b1\u03bd\u03cc\u03c2 \u03bd\u03c4\u03b5 \u03bc\u03b1\u03b4\u03c1\u03af\u03c4\u03b7 de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpa\u0144ski hiszpanii la juan y \u0434\u0435 \u043c\u0430\u0434\u0440\u0438\u0434 \u0438\u0441\u043f\u0430\u043d\u0438\u0438 \u0438\u0441\u043f\u0430\u043d\u0438\u044f \u0438\u0441\u043f\u0430\u043d\u0441\u043a\u0438\u0439 de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae\u03c2 \u03c0\u03bf\u03af\u03b7\u03c3\u03b7 \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae \u03ad\u03c1\u03b3\u03bf \u03c0\u03bf\u03b9\u03b7\u03c4\u03ad\u03c2 \u03c0\u03bf\u03b9\u03ae\u03bc\u03b1\u03c4\u03b1 poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.",
    "P11-1061_aakansha": "Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections Our work is closest to that of Yarowsky and Ngai (2001), but differs in two important ways. The following three sections elaborate these different stages is more detail. We briefly review it here for completeness. It can be shown that this objective is convex in q. However, we do not explore this possibility in the current work. This can be seen as a rough approximation of Yarowsky and Ngai (2001). We tabulate this increase in Table 3. We would like to thank Ryan McDonald for numerous discussions on this topic. Finally, we thank Kuzman Ganchev and the three anonymous reviewers for helpful suggestions and comments on earlier drafts of this paper.",
    "A00-2030_vardha": "A Novel Use of Statistical Parsing to Extract Information from Text Manually creating sourcespecific training data for syntax was not required. For each organization in an article, one must identify all of its names as used in the article, its type (corporation, government, or other), and any significant description of it. Integrated Sentential Processing Post-modifier constituents for the PER/NP. Experimental Results 1 Conclusions The work reported here was supported in part by the Defense Advanced Research Projects Agency. Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062, F30602-97-C-0096, and 4132-BBN-001. We thank Michael Collins of the University of Pennsylvania for his valuable suggestions.",
    "P08-1043_aakansha": "A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing \u201cin the shadow\u201d. Here we do not submit to this view. Parameter Estimation Our smoothing procedure takes into account all the aforementioned aspects and works as follows. Models that employ this strategy are denoted hsp. We currently simulate this by crafting a WCFG and feeding it to BitPar. Experimental Setup Discussion and Conclusion We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal.",
    "D09-1092_sweta": "Polylingual Topic Models The authors thank Limin Yao, who was involved in early stages of this project. This work was supported in part by the Center for Intelligent Information Retrieval, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant number IIS-0326249, and in part by Army prime contract number W911NF-07-1-0216 and University of Pennsylvania subaward number 103548106, and in part by National Science Foundation under NSF grant #CNS-0619337. Any opinions, findings and conclusions or recommendations expressed in this material are the authors\u2019 and do not necessarily reflect those of the sponsor. These tasks can either be accomplished by averaging over samples of \u03a61, . We also remove the twenty-five most frequent word types for efficiency reasons. These tend to be very short, formulaic parliamentary responses, however. Cosine-based rankings are significantly worse. We preprocessed the data by removing tables, references, images and info-boxes. Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission \u03b4\u03b9\u03b1\u03c3\u03c4\u03b7\u03bc\u03b9\u03ba\u03cc sts nasa \u03b1\u03b3\u03b3\u03bb small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimm\u0160inen space lento spatiale mission orbite mars satellite spatial \u05ea\u05d9\u05e0\u05db\u05d5\u05ea \u05d0 \u05e8\u05d5\u05d3\u05db \u05dc\u05dc \u05d7 \u05e5\u05e8 \u05d0\u05d4 \u05dc\u05dc\u05d7\u05d4 spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0441\u043e\u044e\u0437 \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0441\u043f\u0443\u0442\u043d\u0438\u043a \u0441\u0442\u0430\u043d\u0446\u0438\u0438 uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1\u03c2 \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1 de \u03b9\u03c3\u03c0\u03b1\u03bd\u03cc\u03c2 \u03bd\u03c4\u03b5 \u03bc\u03b1\u03b4\u03c1\u03af\u03c4\u03b7 de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpa\u0144ski hiszpanii la juan y \u0434\u0435 \u043c\u0430\u0434\u0440\u0438\u0434 \u0438\u0441\u043f\u0430\u043d\u0438\u0438 \u0438\u0441\u043f\u0430\u043d\u0438\u044f \u0438\u0441\u043f\u0430\u043d\u0441\u043a\u0438\u0439 de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae\u03c2 \u03c0\u03bf\u03af\u03b7\u03c3\u03b7 \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae \u03ad\u03c1\u03b3\u03bf \u03c0\u03bf\u03b9\u03b7\u03c4\u03ad\u03c2 \u03c0\u03bf\u03b9\u03ae\u03bc\u03b1\u03c4\u03b1 poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.",
    "D10-1044_swastika": "Instance Weighting For simplicity, we assume that OUT is homogeneous. This is less effective in our setting, where IN and OUT are disparate. An alternate approximation to (8) would be to let w,\\(s, t) directly approximate p\u02c6I(s, t). We have not explored this strategy. They are: 5We are grateful to an anonymous reviewer for pointing this out. The reference medicine for Silapo is EPREX/ERYPO, which contains epoetin alfa. Le m\u00b4edicament de r\u00b4ef\u00b4erence de Silapo est EPREX/ERYPO, qui contient de l\u2019\u00b4epo\u00b4etine alfa. \u2014 I would also like to point out to commissioner Liikanen that it is not easy to take a matter to a national court. Je voudrais pr\u00b4eciser, a` l\u2019adresse du commissaire Liikanen, qu\u2019il n\u2019est pas ais\u00b4e de recourir aux tribunaux nationaux.",
    "P04-1036_aakansha": "Finding Predominant Word Senses in Untagged Text The paper is structured as follows. We discuss our method in the following section. For each noun we considered the co-occurring verbs in the direct object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations. We trivially labelled all monosemous items. Many of the articles are economy related, but several other topics are included too. The SFC contains an economy label and a sports label. Our approach is complementary to this. Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD. This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies, UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.",
    "P05-1013_aakansha": "Pseudo-Projective Dependency Parsing The rest of the paper is structured as follows. We then evaluate the approach in two steps. If (wi, r, wj) E A, we say that wi is the head of wj and wj a dependent of wi. To explore this tradeoff, we have performed experiments with three different encoding schemes (plus a baseline), which are described schematically in Table 1. In approaching this problem, a variety of different methods are conceivable, including a more or less sophisticated use of machine learning. In the present study, we limit ourselves to an algorithmic approach, using a deterministic breadthfirst search. The results are given in Table 4. All improvements over the baseline are statistically significant beyond the 0.01 level (McNemar\u2019s test). This work was supported in part by the Swedish Research Council (621-2002-4207).",
    "W99-0623_vardha": "Exploiting Diversity in Natural Language Processing: Combining Parsers Two general approaches are presented and two combination techniques are described for each approach. Both parametric and non-parametric models are explored. The theory has also been validated empirically. These two principles guide experimentation in this framework, and together with the evaluation measures help us decide which specific type of substructure to combine. For example, we may have semantic information (e.g. database query operations) associated with the productions in a grammar. The computation of Pfr1(c)1Mi M k (C)) has been sketched before in Equations 1 through 4. This is not an oversight. The first two rows of the table are baselines. This work was funded by NSF grant IRI-9502312.",
    "W99-0623_swastika": "Exploiting Diversity in Natural Language Processing: Combining Parsers Two general approaches are presented and two combination techniques are described for each approach. Both parametric and non-parametric models are explored. The theory has also been validated empirically. These two principles guide experimentation in this framework, and together with the evaluation measures help us decide which specific type of substructure to combine. For example, we may have semantic information (e.g. database query operations) associated with the productions in a grammar. The computation of Pfr1(c)1Mi M k (C)) has been sketched before in Equations 1 through 4. This is not an oversight. The first two rows of the table are baselines. This work was funded by NSF grant IRI-9502312.",
    "A00-2018_akanksha": "A Maximum-Entropy-Inspired Parser * Whenever it is clear to which constituent we are referring we omit the (c) in, e.g., h(c). In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does. Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a, H) for 0 < i < j: Here go(a, H) = 11Z (H) and gi(a, H) = eAi(a,H) fi(\u00b0,11). In many cases this is clearly warranted. (Our experience is that rather than requiring 50 or so iterations, three suffice.) (Actually, we use a minor variant described in [4].) The Experiment The other four equations can be found in a longer version of this paper available on the author's website (www.cs.brown.eduhiec). This is due to the importance of this factor in parsing, as noted in, e.g., [14].",
    "W99-0613_aakansha": "Unsupervised Models for Named Entity Classification Collins We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The first modification \u2014 cautiousness \u2014 is a relatively minor change. Fort= 1,...,T: One implementation issue deserves some elaboration. Again, this deserves further investigation. Unfortunately, modifying the model to account for these kind of dependencies is not at all straightforward. Of these cases, 38 were temporal expressions (either a day of the week or month of the year). We excluded these from the evaluation as they can be easily identified with a list of days/months.",
    "J01-2004_sweta": "This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition  The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling  A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers  A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity  Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model  A small recognition experiment also demonstrates the utility of the model The author wishes to thank Mark Johnson for invaluable discussion, guidance, and moral support over the course of this project. Many thanks also to Eugene Charniak for the use of certain grammar training routines, and for an enthusiastic interest in the project. Thanks also to four anonymous reviewers for valuable and insightful comments, and to Ciprian Chelba, Sanjeev Khudanpur, and Frederick Jelinek for comments and suggestions. Finally, the author would like to express his appreciation to the participants of discussions during meetings of the Brown",
    "J01-2004_aakansha": "This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition  The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling  A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers  A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity  Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model  A small recognition experiment also demonstrates the utility of the model The author wishes to thank Mark Johnson for invaluable discussion, guidance, and moral support over the course of this project. Many thanks also to Eugene Charniak for the use of certain grammar training routines, and for an enthusiastic interest in the project. Thanks also to four anonymous reviewers for valuable and insightful comments, and to Ciprian Chelba, Sanjeev Khudanpur, and Frederick Jelinek for comments and suggestions. Finally, the author would like to express his appreciation to the participants of discussions during meetings of the Brown",
    "P87-1015_swastika": "CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS* Pumping t2 will change only one branch and leave the other branch unaffected. An ATM has two types of states, existential and universal. In an existential state an ATM behaves like a nondeterministic TM, accepting if one of the applicable moves leads to acceptance; in an universal state the ATM accepts if all the applicable moves lead to acceptance. An ATM may be thought of as spawning independent processes for each applicable move. A k-tape ATM, M, has a read-only input tape and k read-write work tapes. M spawns as many processes as there are ways of breaking up ri , .. \u2022 , zt, and rules with A on their left-hand-side. Each spawned process must check if xi , , xn, and , yn, can be derived from B and C, respectively. To do this, the x's and y's are stored in the next 2ni + 2n2 tapes, and M goes to a universal state. Two processes are spawned requiring B to derive z,.., and C to derive yi , , y,.",
    "W06-3114_swastika": "We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back  Evaluation was done automatically using the BLEU score and manually on fluency and adequacy Training and testing is based on the Europarl corpus. To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources. One may argue with these efforts on normalization, and ultimately their value should be assessed by assessing their impact on inter-annotator agreement. So, who won the competition? This actually happens quite frequently (more below), so that the rankings are broad estimates. So, this was a surprise element due to practical reasons, not malice. It rewards matches of n-gram sequences, but measures only at most indirectly overall grammatical coherence. This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No.",
    "P08-1043_swastika": "A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing \u201cin the shadow\u201d. Here we do not submit to this view. Parameter Estimation Our smoothing procedure takes into account all the aforementioned aspects and works as follows. Models that employ this strategy are denoted hsp. We currently simulate this by crafting a WCFG and feeding it to BitPar. Experimental Setup Discussion and Conclusion We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal.",
    "W06-3114_sweta": "We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back  Evaluation was done automatically using the BLEU score and manually on fluency and adequacy Training and testing is based on the Europarl corpus. To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources. One may argue with these efforts on normalization, and ultimately their value should be assessed by assessing their impact on inter-annotator agreement. So, who won the competition? This actually happens quite frequently (more below), so that the rankings are broad estimates. So, this was a surprise element due to practical reasons, not malice. It rewards matches of n-gram sequences, but measures only at most indirectly overall grammatical coherence. This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No.",
    "P11-1061_swastika": "Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections Our work is closest to that of Yarowsky and Ngai (2001), but differs in two important ways. The following three sections elaborate these different stages is more detail. We briefly review it here for completeness. It can be shown that this objective is convex in q. However, we do not explore this possibility in the current work. This can be seen as a rough approximation of Yarowsky and Ngai (2001). We tabulate this increase in Table 3. We would like to thank Ryan McDonald for numerous discussions on this topic. Finally, we thank Kuzman Ganchev and the three anonymous reviewers for helpful suggestions and comments on earlier drafts of this paper.",
    "A00-2018_vardha": "A Maximum-Entropy-Inspired Parser * Whenever it is clear to which constituent we are referring we omit the (c) in, e.g., h(c). In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does. Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a, H) for 0 < i < j: Here go(a, H) = 11Z (H) and gi(a, H) = eAi(a,H) fi(\u00b0,11). In many cases this is clearly warranted. (Our experience is that rather than requiring 50 or so iterations, three suffice.) (Actually, we use a minor variant described in [4].) The Experiment The other four equations can be found in a longer version of this paper available on the author's website (www.cs.brown.eduhiec). This is due to the importance of this factor in parsing, as noted in, e.g., [14].",
    "P04-1036_vardha": "Finding Predominant Word Senses in Untagged Text The paper is structured as follows. We discuss our method in the following section. For each noun we considered the co-occurring verbs in the direct object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations. We trivially labelled all monosemous items. Many of the articles are economy related, but several other topics are included too. The SFC contains an economy label and a sports label. Our approach is complementary to this. Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD. This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies, UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.",
    "P08-1028_sweta": "Vector-based Models of Semantic Composition In fact, the commonest method for combining the vectors is to average them. (1) a. It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. b. Related Work The proposal is not merely notational. Materials and Design The pretest was completed by 53 participants. The study was conducted remotely over the Internet using Webexp4, a software package designed for conducting psycholinguistic studies over the web. A Wilcoxon rank sum test confirmed that the difference is statistically significant (p < 0.01).",
    "D10-1044_aakansha": "Instance Weighting For simplicity, we assume that OUT is homogeneous. This is less effective in our setting, where IN and OUT are disparate. An alternate approximation to (8) would be to let w,\\(s, t) directly approximate p\u02c6I(s, t). We have not explored this strategy. They are: 5We are grateful to an anonymous reviewer for pointing this out. The reference medicine for Silapo is EPREX/ERYPO, which contains epoetin alfa. Le m\u00b4edicament de r\u00b4ef\u00b4erence de Silapo est EPREX/ERYPO, qui contient de l\u2019\u00b4epo\u00b4etine alfa. \u2014 I would also like to point out to commissioner Liikanen that it is not easy to take a matter to a national court. Je voudrais pr\u00b4eciser, a` l\u2019adresse du commissaire Liikanen, qu\u2019il n\u2019est pas ais\u00b4e de recourir aux tribunaux nationaux.",
    "P11-1061_sweta": "Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections Our work is closest to that of Yarowsky and Ngai (2001), but differs in two important ways. The following three sections elaborate these different stages is more detail. We briefly review it here for completeness. It can be shown that this objective is convex in q. However, we do not explore this possibility in the current work. This can be seen as a rough approximation of Yarowsky and Ngai (2001). We tabulate this increase in Table 3. We would like to thank Ryan McDonald for numerous discussions on this topic. Finally, we thank Kuzman Ganchev and the three anonymous reviewers for helpful suggestions and comments on earlier drafts of this paper.",
    "A00-2030_sweta": "A Novel Use of Statistical Parsing to Extract Information from Text Manually creating sourcespecific training data for syntax was not required. For each organization in an article, one must identify all of its names as used in the article, its type (corporation, government, or other), and any significant description of it. Integrated Sentential Processing Post-modifier constituents for the PER/NP. Experimental Results 1 Conclusions The work reported here was supported in part by the Defense Advanced Research Projects Agency. Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062, F30602-97-C-0096, and 4132-BBN-001. We thank Michael Collins of the University of Pennsylvania for his valuable suggestions.",
    "D10-1044_sweta": "Instance Weighting For simplicity, we assume that OUT is homogeneous. This is less effective in our setting, where IN and OUT are disparate. An alternate approximation to (8) would be to let w,\\(s, t) directly approximate p\u02c6I(s, t). We have not explored this strategy. They are: 5We are grateful to an anonymous reviewer for pointing this out. The reference medicine for Silapo is EPREX/ERYPO, which contains epoetin alfa. Le m\u00b4edicament de r\u00b4ef\u00b4erence de Silapo est EPREX/ERYPO, qui contient de l\u2019\u00b4epo\u00b4etine alfa. \u2014 I would also like to point out to commissioner Liikanen that it is not easy to take a matter to a national court. Je voudrais pr\u00b4eciser, a` l\u2019adresse du commissaire Liikanen, qu\u2019il n\u2019est pas ais\u00b4e de recourir aux tribunaux nationaux.",
    "P04-1036_swastika": "Finding Predominant Word Senses in Untagged Text The paper is structured as follows. We discuss our method in the following section. For each noun we considered the co-occurring verbs in the direct object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations. We trivially labelled all monosemous items. Many of the articles are economy related, but several other topics are included too. The SFC contains an economy label and a sports label. Our approach is complementary to this. Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD. This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies, UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship."
}