A Maximum-Entropy-Inspired Parser * Abstract We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.
This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].
The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.
We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.
Conclusion We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.
This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].
That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.
The results reported here disprove this conjecture.
The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.
Indeed, it may be that adding this new parser to the mix may yield still higher results.
From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.
Neither of these results were anticipated at the start of this research.
As noted above, the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing.
Two aspects of this model deserve some comment.
The first is the slight, but important, improvement achieved by using this model over conventional deleted interpolation, as indicated in Figure 2.
We expect that as we experiment with other, more semantic conditioning information, the importance of this aspect of the model will increase.
More important in our eyes, though, is the flexibility of the maximum-entropy-inspired model.
Though in some respects not quite as flexible as true maximum entropy, it is much simpler and, in our estimation, has benefits when it comes to smoothing.
Ultimately it is this flexibility that let us try the various conditioning events, to move on to a Markov grammar approach, and to try several Markov grammars of different orders, without significant programming.
Indeed, we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail.
It is to this project that our future parsing work will be devoted.
As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000).