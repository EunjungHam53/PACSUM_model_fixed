Finding Predominant Word Senses in Untagged Text Abstract word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.
The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data.
Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration.
We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.
The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.
This is a very promising result given that our method does not require any hand-tagged text, such as SemCor.
Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.
Conclusions We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.
We use an automatically acquired thesaurus and a WordNet Similarity measure.
The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.
This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.
In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.
Indeed, the merit of our technique is the very possibility of obtaining predominant senses from the data at hand.
We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.
In the future, we will perform a large scale evaluation on domain specific corpora.
In particular, we will use balanced and domain specific corpora to isolate words having very different neighbours, and therefore rankings, in the different corpora and to detect and target words for which there is a highly skewed sense distribution in these corpora.
There is plenty of scope for further work.
We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al., 2004).
Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.
Whilst we have used WordNet as our sense inventory, it would be possible to use this method with another inventory given a measure of semantic relatedness between the neighbours and the senses.
The lesk measure for example, can be used with definitions in any standard machine readable dictionary.
The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account (McCarthy et al., 2004).
Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41 Table 2: The SENSEVAL-2 first sense on the SENSEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al., 2004) and also because there will be words that occur with insufficient frequency in the hand-tagged resources available.