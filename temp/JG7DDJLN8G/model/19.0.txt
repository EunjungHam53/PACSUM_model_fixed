We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back  Evaluation was done automatically using the BLEU score and manually on fluency and adequacy Conclusions We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.
While many systems had similar performance, the results offer interesting insights, especially about the relative performance of statistical and rule-based systems.
Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.
The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.
The manual evaluation of scoring translation on a graded scale from 1â€“5 seems to be very hard to perform.
Replacing this with an ranked evaluation seems to be more suitable.
Human judges also pointed out difficulties with the evaluation of long sentences.
The official results were slightly better because a lowercase evaluation was used; see (Koehn and Monz, 2006).
We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted of translating Spanish, German, and French texts from and to English.
For our training and test data, we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference.
The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006).
For the bi-text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for French-English (Fr), Spanish-English (Es), and German-English (De) (Koehn and Monz, 2006).
Evaluation results recently reported by Callison-Burch et al.
(2006) and Koehn and Monz (2006) revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator.