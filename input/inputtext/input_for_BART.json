{
    "P87-1015": "CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS* Abstract We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate. In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages. Discussion We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems, and classified these formalisms on the basis of two features: path complexity; and path independence. We contrasted formalisms such as CFG's, HG's, TAG's and MCTAG's, with formalisms such as IG's and unificational systems such as LFG's and FUG's. We address the question of whether or not a formalism can generate only structural descriptions with independent paths. This property reflects an important aspect of the underlying linguistic theory associated with the formalism. In a grammar which generates independent paths the derivations of sibling constituents can not share an unbounded amount of information. The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar, Klein, Pulluna, and Sag, 1985), and GB (as described by Berwick, 1984) with those underlying LFG and FUG. It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's. As illustrated by MCTAG's, it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's, HG's, and TAG's. In order to observe the similarity between these constrained systems, it is crucial to abstract away from the details of the structures and operations used by the system. The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's, HG's, TAG's, and MCTAG's are all local sets. Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently. As suggested in Section 4.3.2, a derivation with independent paths can be divided into subcomputations with limited sharing of information. The approach that Vijay-Shanker et al. (1987) and Weir (1988) take, elaborated on by Becker et al. (1992), is to identify a very general class of formalisms, which they call linear context-free rewriting systems (CFRSs), and define for this class a large space of structural descriptions which serves as a common ground in which the strong generative capacities of these formalisms can be compared. Here we use the standard definition of LCFRS (Vijay-Shanker et al., 1987) and only fix our notation; for a more thorough discussion of this formalism, we refer to the literature. Let G be an LCFRS. There are many (structural) mildly context-sensitive grammar formalisms, e.g., MCFG, LCFRS, MG, and they have been shown to be equivalent (Vijay-Shanker et al., 1987). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987). Following this line, Vijay-Shanker et al. (1987) have introduced a formalism called linear context-free rewriting systems (LCFRSs) that has received much attention in later years from the community. We briefly summarize here the terminology and notation that we adopt for LCFRS; for detailed definitions, see (Vijay-Shanker et al., 1987). We write REGD.k/ to refer to the class of regular dependency languages with a gap-degree bounded by k. Linear Context-Free Rewriting Systems gap-restricted dependency languages are closely related to Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987), a class of formal systems that generalizes several mildly context-sensitive grammar formalisms. This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single-language texts (Kuhlmann and Nivre, 2006). In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987). We briefly summarize the terminology and notation that we adopt for LCFRS; for detailed definitions, see Vijay-Shanker et al. (1987). LCFRS (Vijay-Shanker et al., 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals. A LCFRS (Vijay-Shanker et al., 1987) is a tuple G = (N, T, V, P, S) where a) N is a finite set of non-terminals with a function dim: N \u2192 N that determines the fan-out of each A \u2208 N; b) T and V are disjoint finite sets of terminals and variables; c) S \u2208 N is the start symbol with dim(S) = 1; d) P is a finite set of rewriting rules A (\u2208 1, ... In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes, 1997) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al., 1987). By this result, CCG falls in line with context-free grammars, TAG, and LCFRS, whose sets of derivational structures are all regular (Vijay-Shanker et al., 1987). It is important to note that while CCG derivations themselves can be seen as trees as well, they do not always form regular tree languages (Vijay-Shanker et al., 1987). On this line of investigation, mildly context-sensitive grammar formalisms have been introduced (Joshi, 1985), including, among several others, the tree adjoining grammars (TAGs) of Joshi et al. (1975). Linear context-free rewriting systems (LCFRS), introduced by Vijay-Shanker et al. (1987), are mildly context-sensitive formalisms that allow the derivation of tuples of strings, i.e., discontinuous phrases. CFTG are weakly equivalent to the simple macro grammars of Fischer (1968), which are a notational variant of the well-nested linear context-free rewriting systems (LCFRS) of Vijay-Shanker et al. (1987) and the well-nested multiple context-free grammars (MCFG) of Seki et al. (1991). Thus, CFTG are mildly context-sensitive since their generated string languages are semi-linear and can be parsed in polynomial time (G\u00f3mez-Rodr\u00edguez et al., 2010).",
    "W06-2932": "Multilingual Dependency Analysis with a Two-Stage Discriminative Parser Abstract present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages. The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages. The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph. We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis. Conclusions We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English. In the future we plan to extend these models in two ways. First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling. It is our hypothesis that for languages with fine-grained label sets, joint parsing and labeling will improve performance. Second, we plan on integrating any available morphological features in a more principled manner. The current system simply includes all morphological bi-gram features. It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages. Introduce through post-processing, e.g., through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al., 2006). Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al. (2006) and Nivre et al. (2006). Even though McDonald et al. (2006) and Nivre et al. (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences. The highest score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al. (2006) with a LAS of 87.34 based on the TIGER treebank, but we want to stress that these results are not comparable due to different data sets (and a different policy regarding the inclusion of punctuation). The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP), and labeled F-score (LF). McDonald et al. (2006) use an additional algorithm. Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al., 2007b) and MST Parser (McDonald et al., 2006), two state-of-the-art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and treebanks (McDonald and Nivre, 2007). In fact, our approach can also be applied to other parsers, such as Yamada and Matsumoto's (2003) parser, McDonald et al.'s (2006) parser, and so on. We have shown that, for languages, McDonald et al. (2006) use post-processing for non-projective dependencies and for labeling. As described in (McDonald et al., 2006), we treat the labeling of dependencies as a sequence labeling problem. It should be noted that McDonald et al. (2006) use a richer feature set that is incomparable to our features. Entries marked with * are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al. (2006), Martins et al. (2008), Martins et al. (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different). The specific graph-based model studied in this work is that presented by McDonald et al. (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near-exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation. More precisely, dependency arcs (or pairs of arcs) are first represented by a high-dimensional feature vector f(i, j, l) \u2208 R^k, where f is typically a binary feature vector over properties of the arc as well as the surrounding input (McDonald et al., 2005a; McDonald et al., 2006).",
    "P11-1060": "Learning Dependency-Based Compositional Semantics Abstract Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms. Clarke et al. (2010) and Liang et al. (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al. (2011) present work on unsupervised learning. In particular, Clarke et al. (2010) and Liang et al. (2011) proposed methods to learn from question-answer pairs alone, which represents a significant advance. More recently, Liang et al. (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins. GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al., 2011). Matuszek et al. [2010], Liang et al. [2011], and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world. One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al. 2011) or even a binary correct/incorrect signal (Clarke et al. 2010). For example, Liang et al. (2011) construct a latent parse similar in structure to a dependency grammar, but representing a logical form. Clarke et al. (2010) and Liang et al. (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers. Dependency-based Compositional Semantics (DCS) provides an intuitive way to model the semantics of questions by using simple dependency-like trees (Liang et al., 2011). DCS trees have been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al., 2011) (Figure 1). are explained in 2.5. http://nlp.stanford.edu/software/corenlp.shtml In (Liang et al., 2011) DCS trees are learned from QA pairs and database entries. As in the sentence, \"Tropical storm Debby is blamed for death,\" which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al., 2011) of that variable. Clarke et al. (2010) and Liang et al. (2011) replace semantic annotations in the training set with target answers that are more easily available. and Collins, 2005, 2007), -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al., 2010) systems and DCS (Liang et al., 2011) In general, every plural NP potentially introduces an implicit universal, ranging. For example, Liang et al. (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases explicitly devise quantifier scoping in the semantic model. DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al. (2011) In fact, for any CFG G, see Liang et al. (2011) for work in representing lambda calculus expressions with trees.",
    "E03-1005": "An Efficient Implementation of a New DOP Model Abstract Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank. This paper proposes an integration of the two models which outperforms each of them separately. Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence. Conclusion As our second experimental goal, we compared the models SL-DOP and LS-DOP explained in Section 3.2. Recall that for n=1, SL-DOP is equal to the PCFG-reduction of Bod (2001) (which we also called Likelihood-DOP) while LS-DOP is equal to Simplicity-DOP. Table 2 shows the results for sentences 100 words for various values of n. Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1. Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones. While the first feature has been generally adopted in statistical NLP, the second feature has for a long time been a serious bottleneck, as it results in exponential processing time when the most probable parse tree is computed. This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank. This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000). Data-Oriented Parsing (DOP) methodology is to calculate weighted derivations, but as noted in (Bod, 2003), it is the highest-ranking parse, not derivation, that is desired. Goodman's transform, in combination with a range of heuristics, allowed Bod (2003) to run the DOP model on the Penn Treebank WSJ benchmark and obtain some of the best results obtained with a generative model. Zuidema (2006a) shows that the estimator (Bod, 2003) is also biased and inconsistent, and will, even in the limit of infinite data, not correctly identify many possible distributions over trees. Second, we compare against a composed-rule system, which is analogous to the Data-Oriented Parsing (DOP) approach in parsing (Bod, 2003). Our best-performing model is more accurate than all these previous models except (Bod, 2003). We find that the discriminative probability model is much worse than the generative one, but that training to optimize the discriminative criteria results in improved performance. Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current accuracy level, and only 0.6% below the current best (Bod, 2003). This paper has also proposed a neural network training method which optimizes a discriminative criterion even when the parameters being estimated are those of a generative probability model. Similarly, (Bod, 2003) changes the way frequencies are counted, with a similar effect. (henceforth, STSGs), which can represent single words, contiguous and non-contiguous MWEs, context-free rules, or complete parse trees in a unified representation. My approach is closely related to work in statistical parsing known as Data-Oriented Parsing (DOP), an empirically highly successful approach with labeled recall and precision scores on the Penn Tree Bank that are among the best currently obtained (Bod, 2003). Shown are results where only elementary trees with scores higher than 0.3 and 0.1 respectively are used. However, more interesting is a qualitative analysis of the STSG induced, which shows that, unlike DOP1, push-n-pull arrives at a grammar that gives high weights (and scores) to those elementary trees. We approximated the most probable parse as follows (following (Bod, 2003)). This result is only slightly higher than the highest reported result for this test set, Bod's (.907) (Bod, 2003). This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006), it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well. But equally important is the fact that this new DOP model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003). Of course, it is well-known that a supervised parser's F-score decreases if it is transferred to another domain: for example, the (non-binarized) WSJ-trained DOP model in Bod (2003) decreases from around 91% to 85.5% F-score if tested on the Brown corpus. A moderately larger vocabulary version (4,215 tag-word pairs) of this parser achieves 89.8% F-measure on section 0, where the best current result on the testing set is 90.7% (Bod, 2003). The probability of a parse tree T is the sum of the subtree probabilities, which are adjusted by a simple correction factor discussed in Goodman (2003: 136) and Bod (2003).",
    "W99-0613": "Unsupervised Models for Named Entity Classification Collins Abstract This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- However, we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98). Conclusions Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules. In addition to a heuristic based on decision list learning, we also presented a boosting-like framework that builds on ideas from (Blum and Mitchell 98). The method uses a &quot;soft&quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function. We are currently exploring other methods that employ similar ideas and their formal properties. Future work should also extend the approach to build a complete named entity extractor - a method that pulls proper names from text and then classifies them. The contextual rules are restricted and may not be applicable to every example, but the spelling rules are generally applicable and should have good coverage. The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed. Co-training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web page classification (Blum and Mitchell, 1998), and named entity identification (Collins and Singer, 1999). They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co-Booting). Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick et al. 1999], and so on. (Collins and Singer, 1999) also make use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify. In (Collins and Singer, 1999), Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification. Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named entity recognition. Collins and Singer (1999), for example, report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999). While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al., 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collins and Singer, 1999), and context-free grammar induction (numerous attempts, too many to mention). This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labeled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999). Collins et al. (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by Blum and Mitchell (1998). This approach was shown to perform well on real-world natural language processing problems (Collins and Singer, 1999). (4) prec (p, y) = count (p, y) / count (p) (5) where prec (p, y) is the raw precision of pattern p in the set of documents labeled with category y. Criterion 2: Collins This criterion was used in a lightly supervised NE recognizer (Collins and Singer, 1999). We use Collins and Singer (1999) for our exact specification of Yarowsky. It uses DL rule scores \\( f_j \\) + \\( f_j \\) + \\( f \\) + L (1) where \\( \\lambda \\) is a smoothing constant. This is not clearly specified in Collins and Singer (1999), 3.2 Yarowsky-cautious.",
    "A00-2030": "A Novel Use of Statistical Parsing to Extract Information from Text Abstract Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations. 1 Conclusions We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction. A single model proved capable of performing all necessary sentential processing, both syntactic and semantic. We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required. The semantic training corpus was produced by students according to a simple set of guidelines. This simple semantic annotation was the only source of task knowledge used to configure the model. Section 5 compares our approach to others in the literature, in particular that of (Miller et al., 2000). The basic approach we described is very similar to the one presented in (Miller et al., 2000); however, there are a few major differences: in our approach, the augmentation of the syntactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly. The approach in (Miller The semantic annotation required by our task is much simpler than that employed by Miller et al. (2000). One possibly beneficial extension of our work suggested by (Miller et al., 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level. Similar to the approach in (Miller et al., 2000), we initialized the SLM statistics from the UPenn Treebank parse trees (about 1 M words of training data) at the first training stage, see Section 3. Rule-based methods (Miller et al., 2000) employ a number of linguistic rules to capture relation patterns. One interesting system that does not belong to the above class is that of Miller et al. (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations. Miller et al. (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al. (2000), which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction, and relation extraction in a single model. (Miller et al., 2000) have combined entity recognition, parsing, and relation extraction into a jointly trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly labeled data is unavailable. Miller et al. (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types. Whereas Miller et al. (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance. The syntactic model in (Miller et al., 2000) is similar to Collins, but does not use features like subcat frames and distance measures. Similar to the approach in (Miller et al., 2000) and (Kulick et al., 2004), our parser integrates both syntactic and semantic annotations into a single annotation, as shown in Figure 2. Miller et al. (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels. Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al., 2000), or clustering of semantically similar syntactic dependencies according to their selectional restrictions (Gamallo et al., 2002). This includes parsing and relation extraction (Miller et al., 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al., 2004). For example, Miller et al. (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks. Miller et al. (2000) address the task of relation extraction from the statistical parsing viewpoint. Rule-based methods (Miller et al., 2000) employ a number of linguistic rules to capture relation patterns.",
    "P08-1043": "A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing Abstract Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence. These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance. Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity. Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far. Discussion and Conclusion Employing a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound. The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results. Better grammars are shown here to improve performance on both morphological and syntactic tasks, providing support for the advantage of a joint framework over pipelined or factorized ones. We conjecture that this trend may continue by incorporating additional information, e.g., three-dimensional models as proposed by Tsarfaty and Sima\u2019an (2007). In the current work morphological analyses and lexical probabilities are derived from a small Treebank, which is by no means the best way to go. Using a wide-coverage morphological analyzer based on (Itai et al., 2006) should cater for a better coverage, and incorporating lexical probabilities learned from a big (unannotated) corpus (cf. (Levinger et al., 1995; Goldberg et al., ; Adler et al., 2008)) will make the parser more robust and suitable for use in more realistic scenarios. Acknowledgments We thank Meni Adler and Michael Elhadad (BGU) for helpful comments and discussion. We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal. The work of the first author was supported by the Lynn and William Frankel Center for Computer Sciences. The work of the second author as well as collaboration visits to Israel was financed by NWO, grant number 017.001.271. Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew. Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models. Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMM-based approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging, and parsing of Hebrew, based on lattice parsing. Goldberg and Tsarfaty (2008) propose a generative joint model. Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach. Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text. Following Goldberg and Tsarfaty (2008), we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice. The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew. Models that, in addition, incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing. 4), and in a more realistic one in which parsing and segmentation are handled jointly by the parser (Goldberg and Tsarfaty, 2008) (Sec. It is the same grammar as described in (Goldberg and Tsarfaty, 2008). Several studies followed this line (Cohen and Smith, 2007), the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task. Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the treebank. The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to construct a lattice for each input token. Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parseval to use characters instead of space-delimited tokens as its basic units.",
    "A00-2018": "A Maximum-Entropy-Inspired Parser * Abstract We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head. Conclusion We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100. This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9]. That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it. The results reported here disprove this conjecture. The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought. Indeed, it may be that adding this new parser to the mix may yield still higher results. From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head. Neither of these results were anticipated at the start of this research. As noted above, the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing. Two aspects of this model deserve some comment. The first is the slight, but important, improvement achieved by using this model over conventional deleted interpolation, as indicated in Figure 2. We expect that as we experiment with other, more semantic conditioning information, the importance of this aspect of the model will increase. More important in our eyes, though, is the flexibility of the maximum-entropy-inspired model. Though in some respects not quite as flexible as true maximum entropy, it is much simpler and, in our estimation, has benefits when it comes to smoothing. Ultimately it is this flexibility that let us try the various conditioning events, to move on to a Markov grammar approach, and to try several Markov grammars of different orders, without significant programming. Indeed, we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail. It is to this project that our future parsing work will be devoted. As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000). Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switchboard treebank. We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus. We were interested in the occurrence of features such as type and number of premodifiers, presence and type of postmodifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al., 1997). After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article. The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al., 2003), sentence simplification (Carroll et al., 1999), and a linguist's search engine (Resnik and Elkiss, 2003). In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000). We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis. For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus. The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90% unlabelled and 84% labelled accuracy with respect to dependencies when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows. Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. Pattern-matching approaches were used in (Johnson, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees. As an alternative to hard-coded heuristics, Blaheta and Charniak (2000) proposed recovering the Penn functional tags automatically. The parser of Charniak (2000) is also a two-stage CTF model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions. Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size.",
    "W11-2123": "KenLM: Faster and Smaller Language Model Queries Abstract We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs. The structure uses linear probing hash tables and is designed for speed. Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline. Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations. Future Work There any many techniques for improving language model speed and reducing memory consumption. For speed, we plan to implement the direct-mapped cache from BerkeleyLM. Much could be done to further reduce memory consumption. Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially. Quantization can be improved by jointly encoding probability and backoff. For even larger models, storing counts (Talbot and Osborne, 2007; Pauls and Klein, 2011; Guthrie and Hepple, 2010) is a possibility. Beyond optimizing the memory size of TRIE, there are alternative data structures such as those in Guthrie and Hepple (2010). Finally, other packages implement language model estimation while we are currently dependent on them to generate an ARPA file. While we have minimized forward-looking state in Section 4.1, machine translation systems could also benefit by minimizing backward-looking state. For example, syntactic decoders (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state. If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram, then three or even fewer words could be kept in the backward state. This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible. Exposing this information to the decoder will lead to better hypothesis recombination. Generalizing state minimization, the model could also provide explicit bounds on probability for both backward and forward extension. This would result in better rest cost estimation and better pruning.10 In general, tighter, but well factored, integration between the decoder and language model should produce a significant speed improvement. Conclusion We have described two data structures for language modeling that achieve substantial reductions in time and memory cost. The PROBING model is 2.4 times as fast as the fastest alternative, SRILM, and uses less memory too. The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM. These performance gains transfer to improved system runtime performance; though we focused on Moses, our code is the best lossless option with cdec and Joshua. We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns. The code is opensource, has minimal dependencies, and offers both C++ and Java interfaces for integration. We used common tools for phrase-based translation: Moses (Koehn et al., 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modeling and GIZA++ (Och and Ney, 2000) for word alignments. The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run. Thus, given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) <s> tf, (ii) tf, and (iii) tf </s> and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state information in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states. Our translation system uses cdec (Dyer et al., 2010), an implementation of the hierarchical phrase-based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference. The three data sets in use in this paper are summarized in Table 1. The translation systems consisted of phrase tables and lexicalized reordering tables estimated using the standard Moses (Koehn et al., 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. The features used are basic lexical features, word penalty, and a 3-gram language model (Heafield, 2011). Inference was carried out using the language modeling library described by Heafield (2011). We used the MADA ATB segmentation for Arabic (Roth et al., 2008) and true casing for English, phrases of maximal length 7, Kneser-Ney smoothing, and lexicalized reordering (Koehn et al., 2005), and a 5-gram language model, trained on GigaWord v.5 using KenLM (Heafield, 2011). The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011); both language model implementations are now integrated with Joshua. Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets. With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult-to-compile SRILM toolkit (Stolcke, 2002). This was used to create a KenLM (Heafield, 2011). In the Opinum system, we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application. Our baseline is a factored phrase-based SMT system that uses the Moses toolkit (Koehn et al., 2007) for translation model training and decoding, GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modeling, and minimum error rate training (Och, 2003) to tune model feature weights. For language modeling, we computed 5-gram models using IRSTLM (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011). Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima \u010can, 2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well. 3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using KenLM (Heafield, 2011). n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3. Research efforts to increase search efficiency for phrase-based MT (Koehn et al., 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al., 2006) to additional early pruning techniques (Delaney et al., 2006), (Moore and Quirk, 2007), and more efficient language model (LM) querying (Heafield, 2011). For English language modeling, we use the English Gigaword Corpus with a 5-gram LM using the KenLM toolkit (Heafield, 2011). For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with Kneser-Ney smoothing.",
    "A97-1014": "An Annotation Scheme for Free Word Order Languages Abstract We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- \u2022lar representational strata. Conclusion As the annotation scheme described in this paper focusses on annotating argument structure rather than constituent trees, it differs from existing treebanks in several aspects. These differences can be illustrated by a comparison with the Penn Treebank annotation scheme. The following features of our formalism are then of particular importance: The current tagset comprises only 16 node labels and 34 function tags, yet a. finely grinned classification will take place in the near future. We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order. In general, the resulting interpreted data also are closer to semantic annotation and more neutral with respect to particular syntactic theories. As modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data., interpreted corpora. are a. valuable resource for theoretical and descriptive linguistic research. In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts. Syntactically annotated corpora of German have been missing until now. In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created. We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation. Since the combinatorics of syntactic constructions creates a. demand for very large corpora., efficiency of annotation is an important. criterion for the success of the developed methodology and tools. Our annotation tool supplies efficient manipulation and immediate visualization of argument structures. Partial automation included in the current version significantly reduces the manna.1 effort. Its extension is subject to further investigations. This type of model is used to facilitate the syntactic annotation of the NEGRA corpus of German newspaper texts (Skut et al., 1997). For our experiments, we use the NEGRA corpus (Skut et al., 1997). As data, we use version 2 of the Negra (Skut et al. 1997) treebank, with the common training, devel 1 10 100 1000 10000 100000 3 4 5 6 7 8 9 Frequency Parsing complexity head-driven optimal head-driven Figure 6: The distribution of parsing complexity among productions in Markovized, head-driven grammars read off from NEGRA-25. According to Skut et al. (1997), treebanks have to meet the following requirements: 1 In contrast, some other treebanks, such as the German NeGra and TIGER treebanks, allow annotation with crossing branches (Skut et al., 1997). Non-local dependencies can then be expressed directly by grouping all dependent elements under a single node. Our data source is the German NeGra treebank (Skut et al., 1997). The parsing models we present are trained and tested on the NEGRA corpus (Skut et al., 1997), a hand-parsed corpus of German newspaper text containing approximately 20,000 sentences. The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al., 1997), a syntactically annotated corpus for German. The annotation scheme (Skut et al., 1997) is modeled to a certain extent on that of the Penn Treebank (Marcus et al., 1993), with crucial differences. German is considerably more inflectional, which means that discarding functional information is more harmful, and which explains why the NEGRA annotation has been conceived to be quite so (Skut et al, 1997). The factors used in the algorithms and the algorithms themselves are evaluated on a German corpus annotated with syntactic and coreference information (Negra) (Skut et al, 1997). CKK uses the Dubey and Keller (2003) parser, which is trained on the Negra corpus (Skut et al., 1997). Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al., 1997) report that lexicalization of PCFGs decreases the parsing accuracy when parsing Negra's flat constituent structures. A comparison of unlexicalised PCFG parsing (K\u00fcbler, 2005) trained and evaluated on the German NEGRA (Skut et al., 1997) and the TuBa-D/Z (Telljohann et al., 2004) treebanks using LoPar (Schmid, 2000) shows a difference in parsing results of about 16%, using the PARSEVAL metric (Black et al., 1991).",
    "D09-1092": "Polylingual Topic Models Abstract Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts. Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages. We introduce a polylingual topic model that discovers topics aligned across multiple languages. We explore the model\u2019s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages. Conclusions We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages. We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics. We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora. Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks. When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language. This configuration is similar to PolyLDA (Mimno et al., 2009) or LinkLDA (Yano et al., 2009), such that utterances from different parties are treated as different languages or blog posts and comments pairs. Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). (Mimno et al., 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then adding the Cartesian product of these sets for every topic to a set of candidate translations. Our Wikipedia-based topic similarity feature, w(f, e), is similar in spirit to polylingual topic models (Mimno et al. 2009), but it is scalable to full bilingual lexicon induction. Of the English document and the second half of its aligned foreign language document (Mimno et al., 2009) Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al., 2009) for more details. Evaluation Corpus: The automatic evaluation of cross-lingual coreference systems requires annotated data. 10Mimno et al. (2009) showed that so long as the proportion of topically aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly. Similarly, Polylingual Topic Models (PLTM) (Mimno et al., 2009) generalized LDA to tuples of documents from multiple languages. Our baseline joint PLSA model (JPLSA) is closely related to the polylingual LDA model of (Mimno et al., 2009). We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al., 2009). The difference between the JPLSA model and the polylingual topic model of (Mimno et al., 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al., 2009). Another difference between our model and the polylingual LDA model of (Mimno et al., 2009) is that we use maximum a posteriori (MAP) instead of Bayesian inference. For computing distance, we used the L1 norm of the difference, which worked a bit better than the Jensen-Shannon divergence between the topic vectors used in (Mimno et al., 2009). Documents are defined as speeches by a single speaker, as in (Mimno et al., 2009). For the Wikipedia set, we use 43,380 training documents, 8,675 development documents, and 8,675 final test set documents. For both corpora, the terms are extracted by word-breaking all documents, removing the top 50 most frequent terms, and keeping the next 20,000 most frequent terms. In previously reported work, (Mimno et al., 2009) evaluated parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours. We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing, e.g., polylingual topic models (Mimno et al., 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009). ji = wjk? M j? m k = 1w j k, (1) where M j is the topic distribution of document j and w k is the number of occurrences of phrase pair X k in document j. Mimno et al. (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles). Tuple-specific topic distributions are learned using LDA with distinct topic-word concentration parameters. Mimno et al. (2009) show that PLTM sufficiently aligns topics in parallel corpora. A good candidate for multilingual topic analyses is polylingual topic models (Mimno et al., 2009), which learn topics for multiple languages, creating tuples of language-specific distributions over monolingual vocabularies for each topic. To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al. (2009): add a token-specific language variable and a process for identifying aligned topics. First, polylingual topic models require parallel or comparable corpora in which each document has an assigned language.",
    "D10-1044": "Instance Weighting Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation Abstract We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure. We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines. Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be. The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair\u2019s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction). These estimates are in turn combined linearly with relative-frequency estimates from an in-domain phrase table. Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus. We obtained positive results using a very simple phrase-based system in two different adaptation settings: using English/French Europarl to improve a performance on a small, specialized medical domain; and using non-news portions of the NIST09 training material to improve performance on the news-related corpora. In both cases, the instanceweighting approach improved over a wide range of baselines, giving gains of over 2 BLEU points over the best non-adapted baseline, and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting). In future work we plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion. We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences. Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs. Another popular task in SMT is domain adaptation (Foster et al., 2010). Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al., 2010) and machine translation (Foster et al., 2010). Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now. Yasuda et al. (2008) and Foster et al. (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models. However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs, and merely varies. Our main technical contributions are as follows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al. (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMT translation model: the phrase translation probabilities p(t|s) and p(s|t), and the lexical weights lex(t|s) and lex(s|t). Matsoukas et al. (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al. (2010) extend this approach by weighting individual phrase pairs. These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al. (2010) combine the two, applying linear interpolation to combine the instance-weighted out-of-domain model with an in-domain model. Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE:EN and 1:5 for HT:EN). Previous research has been performed with ratios of 1:100 (Foster et al. 2010) or 1:400 (Axelrod et al. 2011). We expand on work by (Foster et al. 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models. We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research. In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) as well as the linear mixture model (Foster et al., 2010) for conditional phrase-pair probabilities over IN and OUT. For efficiency and stability, we use the EM algorithm to find, rather than L-BFGS as in (Foster et al., 2010). Foster et al. (2010) propose a similar method for machine translation that uses features to capture degrees of generality. As in (Foster et al., 2010), this approach works at the level of phrase pairs. The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al. (2008) and Foster et al. (2010). Foster et al. (2010) do not mention what percentage of the corpus they select for their IR baseline, but they concatenate the data to their in-domain corpus and report a decrease in performance. Foster et al. (2010) further performed this on extracted phrase pairs, not just sentences.",
    "P05-1013": "Pseudo-Projective Dependency Parsing Abstract In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures. We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures. Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy. This leads to the best reported performance for robust non-projective parsing of Czech. Conclusion We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques. The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy, especially with respect to the exact match criterion, leading to the best reported performance for robust non-projective parsing of Czech. Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the nonterminals of constituents during parsing (Nivre and Nilsson, 2005). 1 http://sourceforge.net/projects/mstparser Figure 1: CoNLL-X dependency tree Figure 2: Penn Treebank-style phrase structure tree KSDEP Sagae and Tsujii (2007)'s dependency parser, based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of Nivre and Nilsson (2005) improves accuracy for dependency parsing of Basque. For tree banks with non-projective trees, we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005). It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005). To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson, 2005), in which non-projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non-projective links at parse time. In O(n) time, producing a projective dependency graph satisfying conditions 1-4 in section 2.1, possibly after adding arcs (0, i, lr) for every node i \u2260 0 that is a root in the output graph (where lr is a special label for root modifiers). Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing. To learn transition scores, these systems use discriminative learning methods, e.g., memory-based learning or support vector machines. For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists of lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective. Whereas most of the early approaches were limited to strictly projective dependency structures, where the projection of a syntactic head must be continuous, attention has recently shifted to the analysis of non-projective structures, which are required for linguistically adequate representations, especially in languages with free or flexible word order. The most popular strategy for capturing non-projective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser, as in pseudo-projective parsing (Nivre and Nilsson, 2005), corrective modeling (Hall and Novak, 2005), or approximate non-projective parsing (McDonald and Pereira, 2006). Introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German, and English. Non-projective (Nivre and Nilsson, 2005), we characterize a sense in which the structures appearing in treebanks can be viewed as being only slightly ill-nested. In order to avoid losing the benefits of higher-order parsing, we considered applying pseudo-projective transformations (Nivre and Nilsson, 2005). Pseudo-projective parsing for recovering non-projective structures (Nivre and Nilsson, 2005) Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (Section 3.4). Pseudo-projective parsing was proposed by Nivre and Nilsson (2005) as a way of dealing with non-projective structures in a projective data-driven parser. We projectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r/h, where r is the original label and h is the label of the original head in the non-projective dependency graph. For tree banks with non-projective trees, we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005). Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing in all cases. It should be noted that the proportion of lost dependencies is about twice as high as the proportion of dependencies that are non-projective in themselves (Nivre and Nilsson, 2005). The resulting algorithm is projective, and non-projectivity is handled by pseudo-projective transformations as described in (Nivre and Nilsson, 2005).",
    "P11-1061": "Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections Abstract We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm. Conclusion We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages. Because we are interested in applying our techniques to languages for which no labeled resources are available, we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs. Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data, but have translations into a resource-rich language. Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models. Subramanya et al.'s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers. Fortunately, some recently proposed POS taggers, such as the POS tagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach. Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al., 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith, 2011). Following Das and Petrov (2011) and Subramanya et al. (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics. Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type are unavailable (e.g., Das and Petrov, 2011). Specifically, by replacing fine-grained language-specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features. We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al. (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters. MT-based projection has been applied to various NLP tasks, such as part-of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al. (2007)). There have been two initial attempts to apply projection to create coreference-annotated data for a resource-poor language, both of which involve projecting hand-annotated coreference data from English to Romanian via a parallel corpus. For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2). (Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfer to induce POS tags between two languages. Recent work by Das and Petrov (2011) builds a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text. These approaches build a dictionary by transferring labeled data from a resource-rich language (English) to a resource-poor language (Das and Petrov, 2011). In recent years, research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever-growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactic-semantic (Peirsman and Pado, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al., 2011). Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of-speech tags from the projected part-of-speech tagger of Das and Petrov (2011). This tagger relies only on labeled training data for English and achieves accuracies around 85% on the languages that we consider. In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second, we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language. This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly resourced language to a lesser resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011). Das and Petrov (2011) achieved the current state-of-the-art in unsupervised tagging by exploiting high-confidence alignments to copy tags from the source language to the target language. We have proposed a method for unsupervised POS tagging that performs on par with the current state-of-the-art (Das and Petrov, 2011), but is substantially less sophisticated (specifically not requiring convex optimization or a feature-based HMM).",
    "P08-1028": "Vector-based Models of Semantic Composition Abstract This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments. Discussion In this paper we presented a general framework for vector-based semantic composition. We formulated composition as a function of two vectors and introduced several models based on addition and multiplication. Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here. We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials. Previous applications of vector addition to document indexing (Deerwester et al., 1990) or essay grading (Landauer et al., 1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences. Importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components. The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately. Further research is needed to gain a deeper understanding of vector composition, both in terms of modeling a wider range of structures (e.g., adjectivenoun, noun-noun) and also in terms of exploring the space of models more fully. We anticipate that more substantial correlations can be achieved by implementing more sophisticated models from within the framework outlined here. In particular, the general class of multiplicative models (see equation (4)) appears to be a fruitful area to explore. Future directions include constraining the number of free parameters in linguistically plausible ways and scaling to larger datasets. The applications of the framework discussed here are many and varied both for cognitive science and NLP. We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al., 1988) and inductive inference (Heit and Rubinstein, 1994). NLP tasks that could benefit from composition models include paraphrase identification and context-dependent language modeling (Coccaro and Jurafsky, 1998). Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p + a as a function f operating on four components: c = f(p, a, R, K) (3) R is the relation holding between p and a, and K is additional knowledge. Mitchell and Lapata (2008), henceforth M&L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression. Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting. And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors. Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p = f(u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is the syntactic context, K is a semantic knowledge base, and p is the resulting composed vector (or tensor). As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now. Mitchell and Lapata (2008) propose a framework to define the composition c = f(a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition. As our final set of baselines, we extend two simple techniques proposed by Mitchell and Lapata (2008) that use element-wise addition and multiplication operators to perform composition. Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work, we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008). Assuming that h is a linear function of the Cartesian product of u and v allows us to specify additive models which are by far the most common method of vector combination in the literature: hi = ui + vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi = ui * vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well-known tensor products (Smolensky 1990) and circular convolution (Plate 1995). Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and pointwise multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001). The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression. For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset. Mitchell and Lapata (2008) observed that a simple multiplication function modeled compositionality better than addition. We use the compositionality functions, simple addition and simple multiplication, to build compositional vectors Vwr1 + wr2 and Vwr1 ? wr2. These are as described in (Mitchell and Lapata, 2008).",
    "P08-1102": "Cascaded Linear Model A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging Abstract We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging. With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly. Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline. Conclusions We proposed a cascaded linear model for Chinese Joint S&T. Under this model, many knowledge sources that may be intractable to be incorporated into the perceptron directly, can be utilized effectively in the outside-layer linear model. This is a substitute method to use both local and non-local features, and it would be especially useful when the training corpus is very large. However, can the perceptron incorporate all the knowledge used in the outside-layer linear model? If this cascaded linear model were chosen, could more accurate generative models (LMs, word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely, or by self-training on raw corpus in a similar approach to that of McClosky (2006)? In addition, all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus, whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low, 2004). How can we utilize these knowledge sources effectively? We will investigate these problems in the following work. Following Jiang et al. (2008), we describe segmentation and Joint S&T as below: For a given Chinese sentence appearing as a character sequence: C1: n = C1 C2. As described in Ng and Low (2004) and Jiang et al. (2008), we use s to indicate a single character word, while b, m, and e indicate the begin, middle, and end of a word respectively. Plates called lexical targets in the column below are introduced by Jiang et al. (2008). Jiang et al. (2008) propose a cascaded linear model for joint Chinese word segmentation and POS tagging. We use the feature templates the same as Jiang et al. (2008) to extract features from the model. Approach, where basic processing units are characters that compose words (Jiang et al., 2008a). 6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al., 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results. 6.1.2 Lattice-Forest System We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al., 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm. However, when we repeat the work of (Jiang et al., 2008), which reports achieving state-of-the-art performance in the datasets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper). Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appear twice, which are generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al., 2008), with n=0, generates [C0C0]) As all the features adopted in (Jiang et al., 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle. Last, (Jiang et al., 2008) adds repeated features implicitly based on (Ng and Low, 2004). Previous joint models mainly focus on word segmentation and POS tagging tasks, such as the virtual nodes method (Qian et al., 2010), cascaded linear model (Jiang et al., 2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), and re-ranking (Jiang et al., 2008b).",
    "J01-2004": "This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition  The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling  A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers  A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity  Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model  A small recognition experiment also demonstrates the utility of the model Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark; see Roark (2001)). Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a treebank, with the exception of Chelba and Jelinek (2000). The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn Treebank. We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn Treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search. One way around this problem is to adopt a two-pass approach, where GEN(x) is the top N analyses under some initial model, as in the reranking approach of Collins (2000). In the current paper, we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999). The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights. Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word. A good example of this is the Roark parser (Roark, 2001), which works left-to-right through the sentence and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word and then re-pruning. We ran the first stage parser with 4-times over parsing for each string in the 7 best lists provided by Brian Roark (Roark, 2001). A local tree is an explicit expansion of an edge and its children. Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001). In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here. At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures. Let H(D) be the entropy over a set of derivations D, calculated as follows: H(D) = \u03a3 D (D) P(D) log P(D) (10) If the set of derivations D = D(G, W[1, i]) is a set of partial derivations for string W[1, i], then H(D) is a measure of uncertainty over the partial derivations, i.e., the uncertainty regarding the correct analysis of what has already been processed.",
    "P04-1036": "Finding Predominant Word Senses in Untagged Text Abstract word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data. Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration. We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically. The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task. This is a very promising result given that our method does not require any hand-tagged text, such as SemCor. Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora. Conclusions We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet. We use an automatically acquired thesaurus and a WordNet Similarity measure. The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task. This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor. In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus. Indeed, the merit of our technique is the very possibility of obtaining predominant senses from the data at hand. We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns. In the future, we will perform a large scale evaluation on domain specific corpora. In particular, we will use balanced and domain specific corpora to isolate words having very different neighbours, and therefore rankings, in the different corpora and to detect and target words for which there is a highly skewed sense distribution in these corpora. There is plenty of scope for further work. We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al., 2004). Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses. Whilst we have used WordNet as our sense inventory, it would be possible to use this method with another inventory given a measure of semantic relatedness between the neighbours and the senses. The lesk measure for example, can be used with definitions in any standard machine readable dictionary. The first sense heuristic, which is often used as a baseline for supervised WSD systems, outperforms many of these systems that take surrounding context into account (McCarthy et al., 2004). Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41 Table 2: The SENSEVAL-2 first sense on the SENSEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al., 2004) and also because there will be words that occur with insufficient frequency in the hand-tagged resources available. The method is described in (McCarthy et al., 2004), which we summarize here. McCarthy et al. (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD. We build upon this previous research and propose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges. Previous research in inducing sense rankings from an untagged corpus (McCarthy et al., 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction. McCarthy et al. (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus. Research by McCarthy et al. (2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn. In addition, we implemented the unsupervised method of (McCarthy et al., 2004), which calculates a prevalence score for each sense of a word to predict the predominant sense. McCarthy et al. (2004) reported that the best results were obtained using k = 50 neighbors and the WordNet Similarity JCN measure (Jiang and Conrath, 1997). In doing so, we provide first results on the application of WordNet automatic sense ranking (ASR) to French parsing, using the method of McCarthy et al. (2004). To define an appropriate categorical distribution over synsets for each lemma x in our source vocabulary, we first use the WordNet resource to identify the set Sx of different senses of x. We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each sense s \u2208 Sx, following the approach of McCarthy et al. (2004). As explained in Section 2.2, ASR is performed using the method of McCarthy et al. (2004) This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al., 2004). More radical solutions than sense grouping that have been proposed are to restrict the task to determining the predominant sense in a given domain (McCarthy et al., 2004), or to work directly with paraphrases (McCarthy and Navigli, 2009). In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by WordNet, resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al. (2004)). It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al., 2004). The first, most frequent sense (MFS) (McCarthy et al., 2004), is a widely used baseline for supervised WSD systems.",
    "W99-0623": "Exploiting Diversity in Natural Language Processing: Combining Parsers Abstract Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy. Two general approaches are presented and two combination techniques are described for each approach. Both parametric and non-parametric models are explored. The resulting parsers surpass the best previously published performance results for the Penn Treebank. Conclusion We have presented two general approaches to studying parser combination: parser switching and parse hybridization. For each experiment we gave an nonparametric and a parametric technique for combining parsers. All four of the techniques studied result in parsing systems that perform better than any previously reported. Both of the switching techniques, as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments. Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result. Combining multiple highly-accurate independent parsers yields promising results. We plan to explore more powerful techniques for exploiting the diversity of parsing methods. 1 Introduction Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy. The collection of hypotheses ti = fi (Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999) 5 (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers. A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999). This approach roughly corresponds to (Henderson and Brill, 1999)'s Na\u00efve Bayes parse hybridization. Henderson and Brill (1999) also reported that context did not help them outperform simple voting. (Henderson and Brill, 1999) improved their best parser's F-measure from 89.7 to 91.3, using their naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction). Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren et al., 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pedersen, 2000). Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes: one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees. Henderson and Brill (1999) combine three parsers and obtain an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper. Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006). Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents of the initial trees. (Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined. (Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents. Output (Figure 3). Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected F-score within the Minimum Bayes Risk (MBR) framework. Third, we extend these parser combination methods from 1-best outputs to n-best outputs. System combination has benefited various NLP tasks in recent years, such as products of experts (e.g., Smith and Eisner, 2005) and ensemble-based parsing (e.g., Henderson and Brill, 1999). In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). Henderson and Brill (1999) perform parse selection by maximizing the expected precision of selected parses with respect to the set of parses to be combined.",
    "W06-3114": "We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back  Evaluation was done automatically using the BLEU score and manually on fluency and adequacy Conclusions We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs. While many systems had similar performance, the results offer interesting insights, especially about the relative performance of statistical and rule-based systems. Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics. The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data. The manual evaluation of scoring translation on a graded scale from 1\u20135 seems to be very hard to perform. Replacing this with an ranked evaluation seems to be more suitable. Human judges also pointed out difficulties with the evaluation of long sentences. The official results were slightly better because a lowercase evaluation was used; see (Koehn and Monz, 2006). We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted of translating Spanish, German, and French texts from and to English. For our training and test data, we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference. The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006). For the bi-text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for French-English (Fr), Spanish-English (Es), and German-English (De) (Koehn and Monz, 2006). Evaluation results recently reported by Callison-Burch et al. (2006) and Koehn and Monz (2006) revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. For instance, Callison-Burch et al. (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al., 2001). We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al. (2006) (see Section 3). We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al. (2006) We use the same method described in (Koehn and Monz, 2006) to perform the significance test. We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006). The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002). Callison-Burch et al. (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. The English-German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006). We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006). A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006). For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006). The correlations on the document level were computed on the English, French, Spanish, and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al., 2007), and third shared translation task (Callison-Burch et al., 2008)."
}